%\begin{comment}
\begin{table*}[t]
\small
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}lrrrrrrrrr@{}}
\toprule
\textbf{Dataset} & \textbf{\ctrltag{negation}} & \textbf{\ctrltag{quantifier}} & \textbf{\ctrltag{lexical}} & \textbf{\ctrltag{resemantic}} & \textbf{\ctrltag{insert}} & \textbf{\ctrltag{delete}} & \textbf{\ctrltag{restructure}} & \textbf{\ctrltag{shuffle}} & \emph{\ctrltag{global}} \\ 
\midrule
        CAD &      3,274 &         292 &    8,143 &         2,603 &      960 &         952 &          220 &       36 &    3,466 \\
   Contrast &       336 &         436 &     1,607 &         1,291 &      589 &         586 &          275 &      149 &     877 \\
       HANS &        50 &           0 &        0 &              0 &      3,926 &       3,926 &          494 &     1,602 &       2 \\
    ParaNMT &      2,797 &         825 &    10,000 &        10000 &    6,442 &       6,205 &         5,136 &     1,417 &   10,000 \\
       PAWS &        81 &        1,815 &    10,000 &        10000 &    3,630 &       3,403 &         4,551 &    10,000 &   10,000 \\
 WinoGrande &      3,011 &          94 &    10,000 &        6,927 &     120 &         124 &          453 &       65 &    3184 \\
    \emph{Crawled} &         0 &           0 &     5,000 &           0 &    5,000 &    5,000 &            0 &      108 &    5,000 \\
      \textbf{Total} &      9,549 &        3,462 &    44,750 &       30,821 &   20,667 &   20,167 &        11,129 &    13,377 &   32,529 \\
\bottomrule
\vspace{-15pt}
\end{tabular}
\caption{The datasets used for finetuning \sysname, and the \tagstr distributions.}
\label{table:gpt_train_stats}
\vspace{-10pt}
\end{table*}



\section{GPT-2 as Counterfactual Generator}
\label{appendix:train_data}

\subsection{Training Data and Parameters}

We combine several datasets to finetune \sysname.

\textbf{Contrast set.}
Authors of 10 existing NLP dataset each manually perturbed 100--1,000 instances to change the gold label, so to inspect a model's local decision boundary~\cite{gardner2020contrast}.
The perturbation patterns vary based on the tasks and the annotators, allowing us to learn diverse strategies.
To make sure we can use the contrast set to evaluate the \sst model, we excluded the IMDb movie review from the training.
%\wts{Re-train the model with other contrast sets.}


\textbf{Counterfactually-augmented data (CAD).}
\citet{kaushik2019learning} crowdsourced counterfactuals for IMDb movie review (1.7k), which we split into paired sentences to match the text length of other datasets.
CAD's perturbation patterns also vary based on the task, but can especially contribute to \ctrltag{negation}.
As NLI is in our demonstrating applications, we did not use their 6.6k SNLI counterfactuals.\footnote{Similarly, though \qqp is suitable for training \sysname, we omitted it so \qqp can be used in our evaluation.}
% and SNLI (6.6k).


\textbf{WinoGrande} is a large-scale dataset of 44k instances for testing common sense problems~\cite{sakaguchi2019winogrande}.
It contains sentences that differ only by one trigger word (\eg one noun), making it most suitable for learning lexical exchanges.

\textbf{ParaNMT-50M} contains 50 million English-English sentential paraphrase pairs, covering various domains and styles of text, as well as different sentence structures~\cite{wieting2017paranmt}. 

\textbf{PAWS}~\cite{zhang2019paws} contains pairs with high text overlaps, created through controlled word swapping, best demonstrating \ctrltag{shuffle} and \ctrltag{restructure}. We used its 49k Wikipedia parts.


\textbf{HANS}~\cite{mccoy2019right}, a challenge set for NLI, contains 10k pairs of premises and hypotheses created based on 10 heavily fallible syntactic templates, and therefore compensates rarer structural changes that may be missed by PAWS.


\textbf{Crawled} 
We additionally crawl naturally occurring sentence pairs from non-paired datasets boost some specific patterns and increase lexical diversity. 
This include 
(1) CommonGen~\cite{lin-etal-2020-commongen}, sentences with common sense concepts; 
(2) Natural Questions~\cite{kwiatkowski-etal-2019-natural}, collections of queries issued to Google Engines (and therefore involve various paraphrases of similar user intents), and 
(3) SQuAD \cite{rajpurkar-etal-2016-squad}, whose paragraphs involve Wikipedia knowledge.
We estimate \emph{close} pairs using edit distance, and broadly accept those with less than 60\% editing.
To exclude tricky cases (\eg \exinline{how do I not be} can be incorrectly regarded as \ctrltag{negation} for \exinline{how do I recover it}), we only augment the most determined patterns: \ctrltag{lexical}, \ctrltag{insert}, \ctrltag{delete}, and \ctrltag{shuffle}.
%Among them, we further filter the pairs using \tagstrs (see the section below).
%Still, unrelated examples 

\begin{comment}
\subsection{Training Prompts \& Parameters}
\tofix{This part is mostly moved to the main paper.}
We use a pair of two sentences interchangeably as $x$ and $\xp$ to learn the \tagstrs both ways.
For each $(x, \xp)$ pair, we compute its primary \tagstr using part-of-speech tags and dependency trees, and blank out the changed subtrees in $\xp$.
For example, \ctrltag{negation} occurs when we observe changes to negation modifiers or specific words like ``supposedly'', and \ctrltag{shuffle} occurs when we have overlaps between tokens deleted and added.
When multiple changes occur, we label it with the primary \tagstr, which most significantly changes the semantic meaning on the corresponding subphrase.
In Figure~\ref{fig:blank}A, we use \ctrltag{negation}, as \swap{great}{not great} is more significant than \swap{kids}{children}.
If we cannot identify the \tagstr for a $(x, \xp)$ or if the editing distance is too large, we denote it with \ctrltag{global} and use it as a negative training sample.
To balance the distribution (Table~\ref{table:gpt_train_stats}), for each dataset, we extract \tagstrs from all the $(x, \xp)$, and randomly sample up to 10,000 instances per \tagstrshorts.
\fixed{Still, in Table~\ref{table:gpt_train_stats} , \ctrltag{quantifier} and \ctrltag{negation} have less training data compared to other codes. 
Fortunately, these codes tend to be limited to more specific patterns (``more than'', ``not'', ``never'') when compared to “broad” codes like \ctrltag{lexical}, and thus even a small sample is enough to learn them. }


To allow flexible blanking at generation time, we generate multiple training prompts for each $(x, \xp)$, by blanking (1) just the changed tokens, (2) the associated parsing structures, (3) the merged changes, and (4) the entire sentence (examples are in Figure~\ref{fig:blank}).
%As a result, we form up to four unique training prompts given one $(x, \xp)$ pair 
%max_log_prob_diff
%With the interchangeable orders and the blanks, we generate 657,144 training prompts from 191,415 sentence pairs.
%\wts{Add the training hyperparameters; And maybe eventually move them to appendix.}
\end{comment}

\fixed{To balance the distribution (Table~\ref{table:gpt_train_stats}), for each dataset, we extract \tagstrs from all the $(x, \xp)$, and randomly sample up to 10,000 instances per \tagstrshorts.
Still, \ctrltag{quantifier} and \ctrltag{negation} have less training data compared to other codes. 
Fortunately, these codes tend to be limited to more specific patterns (``more than'', ``not'', ``never'') when compared to ``broad'' codes like \ctrltag{lexical}, and thus even a small sample is enough to learn them. }
We finetuned an off-the-shelf GPT-2 model from \citet{Wolf2019HuggingFacesTS} for 10 epochs with an initial learning rate 5e-5, a batch size of 8, and a sequence length of 120 (but any LM can potentially be used).
We select the best epoch based on the evaluation loss on a holdout set of size 5,000.
The training took around 8 hours on two Titan RTXs.


\subsection{Intrinsic Evaluation Details}
\label{appendix:intrinsic}


\subsubsection{Closeness and Diversity}
\label{appendix:closeness}
Similar to \citet{madaan2020generate}, we compare the \emph{diversity} and \emph{closeness} of \sysname with alternative generators, \ie RoBERTa and T5, representing masked language models that prioritize word and span substitution, and original \emph{GPT-2}, representing the standard generative model not conditioned on $x$. 
For a given $x$ and its counterfactuals $\hat{\xset}$, we approximate \emph{diversity} using self-BLEU~\cite{zhu2018texygen} within $\hat{\xset}$.
Meanwhile, \emph{closeness} is the average distance between $x$ and every $\xp \in \hat{\xset}$, both with the normalized word level Levenshtein edit distance (\cite{levenshtein1966binary}, used in MiCE~\cite{ross2020explaining}), and syntactic tree edit distance (\cite{zhang1989simple} in GYC~\cite{madaan2020generate}).

We run the three generators on 300 sentences in total.
In GPT-2, we take the first two words of an $x$ as the input context (prompt), limit the length of the generation to be similar to $x$, and collect 10 counterfactuals.
As for RoBERTa and T5, we repeatedly perturb $x$ for three times, each time randomly placing up to three \texttt{[MASK]} tokens, and ask the generator to generate 5 counterfactuals through beam search, following \citet{checklist:acl20}.
\sysname uses the same blank (mask) placement as in RoBERTa and T5, but we additionally enumerate through all \tagstrs.
For each $x$, we randomly sample 5 counterfactuals to form $\hat{\xset}$ per generator.

As shown in Table~\ref{table:intrinsic}, \sysname achieves a balance between diversity and closeness.
Ideally, we would also like to compare \sysname with concurrent work \cite{madaan2020generate, ross2020explaining}, but these are yet to be open-sourced and require extensive implementation or finetuning.
%Inspired by style transfer~\cite{yang2018unsupervised} and controlled text generation, GYC performs the perturbation on the latent space of the input $x$.
%Meanwhile, MiCE uses a two-step framework to generate counterfactual explanations, with the generator being T5~\cite{JMLR:v21:20-074} finetuned on the task-specific dataset.
%As mentioned in \S\ref{sec:relate}, both generators focus on flipping the class label of a given $x$.
% Unfortunately, both require extensive implementation or finetuning, and has yet to be opensourced.

%Still, we believe that our infilling (\texttt{BLANK}) structure will improve the \emph{closeness} over the unconstrained GYC (called content and syntactic preservation in their case).
%We also hypothesize that \sysname has better (or at least compatible) diversity than GYC. 
%Per their intended use case --- model testing and debiasing --- the control functions in GYC are driven by the predictor-of-interest (\eg sentiment classifier, NER tagger). 
%As a result, most of the GYC changes seem to focus on features deemed important by the classifier (\eg \exinline{I am very disappointed with the service} is changed by \swap{disappointed}{pleased}, \swap{disappointed}{happy}, \swap{disappointed with}{pleased to get a good}).

\subsubsection{Controllability}
\label{appendix:ablation_control}

\fixed{To evaluate controllability, we compare \sysname with T5, and GPT-2 finetuned on prompts \emph{without} \tagstrshorts (called \sysname\emph{-a}), such that both baselines consider sufficient context.}
%We finetune GPT-2 on training prompts that \emph{do not} contain \tagstrs (called \emph{\sysname-a}), and quantify the impact of the \tagstrshorts through an ablation study.
For each \tagstr, we compare the \emph{control success rate} of \sysname and \sysname-a on 300 prompts.
For each prompt, we generate counterfactuals through beam search (beam $=5$), and recompute the \tagstrshorts on the top three generated $\xp$.
We deem the control successful if at least one of the three recomputed \tagstrshorts matches the desired control code (though in \sysname-a, we only measure whether the \tagstrshort naturally occurs in the uncontrolled generation.)
The success rate increases by $26\% \pm 13\%$ across all \tagstrs, ranging from \ctrltag{quantifier} (increasing 6\%, from 50\% to 56\%) to \ctrltag{negation} (42\%, from 5\% to 47\%).
\fixed{Non-finetuned T5 also achieves less control (success rate decreases by 33\% on average.)}

%\ctrltag{lexical} has the smallest increment \tofix{from $93\%$ to $100\%$}, mostly because \emph{\sysname-a} tend to frequently replace words.
%\ctrltag{insert} is the most impactful \tagstrshort (from \tofix{$12\%$ to $100\%$}) --- \sysname-a rarely insert additional clues on its own.


Common failure cases include
%Note that all the prompts used are guaranteed to allow the corresponding \tagstrs, and the \tagstrshorts can be less effective on more general prompts:
(1) The \tagstrs conflict with the blanks, \eg \exinline{a dog is embraced by a \texttt{[BLANK]}} would not respond to \ctrltag{negation}.
(2) $x$ does not have a corresponding pattern, \eg \ctrltag{shuffle} is not applicable to \exinline{the movie is good.}
(3) certain salient patterns dominate the generation probability, \eg the model tends to perturb the quantifier ``two'' in \exinline{two dogs are running,} regardless of the \tagstrshort.
%In the ablation study, we filtered out prompts that fell under cases 1 and 2.




\begin{comment}
{'gpt2': {'bleu4': 0.17553428639447374,
  'bluescore': 0.8947643,
  'sem_dist': 0.6225155562821245,
  'tree_dist': 6.350782997762864,
  'edit_dist': 0.7045303072049199},
 'bert': {'bleu4': 0.4699445059841606,
  'bluescore': 0.9546045,
  'sem_dist': 0.15153456281610847,
  'tree_dist': 1.352,
  'edit_dist': 0.14362360165631063},
 'polyjuice': {'bleu4': 0.33819550232972273,
  'bluescore': 0.9369372,
  'sem_dist': 0.23683031172394833,
  'tree_dist': 2.1298870056497172,
  'edit_dist': 0.2505382626667885}}
\end{comment}




