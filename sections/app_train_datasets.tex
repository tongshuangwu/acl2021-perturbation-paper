\section{Datasets for GPT-2 Finetuning}
\label{appendix:train_data}

%\begin{comment}
\begin{table*}
\small
\centering
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{lrrrrrrrrr}
\toprule
\textbf{Dataset} & \textbf{\ctrltag{negation}} & \textbf{\ctrltag{quantifier}} & \textbf{\ctrltag{leixcal}} & \textbf{\ctrltag{resemantic}} & \textbf{\ctrltag{insert}} & \textbf{\ctrltag{delete}} & \textbf{\ctrltag{restructure}} & \textbf{\ctrltag{shuffle}} & \emph{\ctrltag{global}} \\ 
\midrule
        CAD &      3418 &         435 &     7335 &        3896 &    2430 &    2432 &         3872 &       82 &    3702 \\
    Crawled &         0 &           0 &     5000 &           0 &    5000 &    5000 &            0 &       91 &    5000 \\
   contrast &       314 &         294 &     1180 &        1167 &     575 &     575 &         1085 &      113 &     843 \\
       hans &        46 &           0 &        0 &           0 &    3893 &    3891 &          791 &     1180 &     199 \\
    paranmt &      2201 &         513 &     5262 &       10000 &    5446 &    5400 &        10000 &     1592 &   10000 \\
       paws &        61 &         284 &     5672 &        2732 &    4161 &    4155 &        10000 &    10000 &   10000 \\
 winogrande &      3134 &         116 &    10000 &        7612 &     169 &     170 &         4271 &      233 &    3395 \\
      total &      9174 &        1642 &    34449 &       25407 &   21674 &   21623 &        30019 &    13291 &   33139 \\
\bottomrule
\end{tabular}
\caption{The datasets used for finetuning the GPT-2 perturbation model, and the \tagstr distributions.}
\label{table:gpt_train_stats}
\end{table*}
%\end{comment}


We combined the following NLP datasets in finetuning our GPT-2 perturbation model.
To achieve a more balanced distribution, for each dataset, we extract \tagstrs from all the data pairs available, and randomly sample up to 10k instances per \tagstr.
The distribution is shown in Table~\ref{table:gpt_train_stats}.

\paragraph{Contrast set}
In \cite{gardner2020contrast}, authors of 10 existing NLP dataset manually each perturbed 100-1,000 test instances in small but meaningful ways that change the gold label, so to inspect a model's decision boundary around a local instance.
\wts{Make sure NLP is expanded somewhere in intro}
The perturbation pattern varies based on the tasks and the annotators, allowing us to learn diverse perturbation methods.
To make sure we can use the contrast set to evaluate the sentiment analysis model, we excluded the IMDB movie review from the training.
\footnote{Similarly, though QQP would be a potentially interesting dataset for training the perturbation model, we omitted it so QQP can be used in our evaluation.}
%\wts{Re-train the model with other contrast sets.}


\paragraph{Counterfactually-augmented data (CAD)}
To augment the training data, \citet{kaushik2019learning} crowdsourced counterfactual perturbations for IMDB movie review (1.7k perturbations on 1.7k original instances) and SNLI (6.6k perturbations on 1.67k original instances).
Similar to contrast set, the perturbation patterns vary based on the task, but can especially contribute to \ctrltag{negation}.
We split the movie review paragraphs into paired sentences, to match the sentence length of other datasets.


\paragraph{WINOGRANDE} is a large-scale dataset of 44k problems for testing common sense problems~\cite{sakaguchi2019winogrande}.
The dataset contains nearly identical sentences that differ only by one trigger word, which flips the correct answer choice for certain questions.
The dataset is most suitable for lexical tokens that are suitable for similar use cases.

\paragraph{ParaNMT-50M} contains 50 million English-English sentential paraphrase pairs, covering various domains and styles of text, as well as different sentence structures~\citet{wieting2017paranmt}. 

\paragraph{PAWS} contains paraphrase and non-paraphrase pairs with high lexical overlap. 
\citet{zhang2019paws} created 108k challenging pairs by controlled word swapping and back translation.
As a result, the dataset demonstrates the \ctrltag{shuffle} and \ctrltag{restructure} strategy.


\paragraph{HANS} is a controlled evaluation dataset designed for testing decision boundaries of NLI models~\cite{mccoy2019right}. 
The dataset contains 10k pairs of premises and hypotheses created based on 10 heavily on fallible syntactic heuristic rules, and therefore compensates rarer structural changes that may be missed by PAWS.