\section{Task Formalization \& Modeling}

\begin{comment}
* Define the task
- Some math
- Several different application domains
    - augmentation
    - adversarial attack
    - extend data
    - counterfactual explanation
- Importance of where to change and how to change

* How to train
- Compute control tags
- Special tokens

* Use existing datasets
- Why each dataset 
- Data distribution [maybe appendix]

* Training hyperparameters

* Evaluations
- Filtering
- Diversity
\end{comment}
\subsection{Definitions, Applications, \& Properties}

\newcommand{\xset}{\mathbf{X}}
\newcommand{\yset}{\mathbf{Y}}
\newcommand{\gset}{\mathbf{G}}

\newcommand{\dset}{\mathcal{D}}
\newcommand{\xp}{\hat{x}}
\newcommand{\yp}{\hat{y}}

\begin{comment}
% Originally wanted a table to summarize 
\begin{table}
\small
\centering
\begin{tabular}{r c c}
\toprule

Application & \textbf{$y = \yp$} & $f(x)=f(\xp)$ \\ 
\midrule
Adversarial training 		& \cmark & \qmark \\
Data augmentation  			& \qmark & \qmark \\
Perturbation explanation 	& \qmark & \qmark \\
Adversarial training 		& \cmark & \qmark \\
\bottomrule
\end{tabular}

\caption{The datasets used for finetuning the GPT-2 perturbation model, and the \tagstr distributions.}
\label{table:gpt_train_stats}
\end{table}
\end{comment}



Suppose $\mathcal{X}$, $\mathcal{Y}$ denote the instance and label spaces of a dataset $\dset = \{ (x,y) | x \in \xset, y \in \yset \}\subset \mathcal{X} \times \mathcal{Y}$. We define a perturbation to be a mapping from an \emph{original} instance to a new, \emph{perturbed} one $\xp$.
The perturbation follow certain \emph{strategies} (e.g., negations~\cite{kaushik2019learning}, word substitution~\cite{li-etal-2020-bert-attack}, syntactical restructing~\cite{iyyer2018adversarial}).
The edited spans --- including words removed from $x$ or newly added to $\xp$, and the affected parsing tree structures --- are instantiations of the strategies.
For example, \swap{did}{didn't}, \swap{would never}{would} are both instantiations for \ctrltag{negation}.
We aim to design a counterfactual generator $g$, such that for a given original instance $x$, we can gather multiple $\xp$ for the following applications, with desired properties.

\textbf{Applications.}
Perturbed examples have been used for training, evaluating, and explaining models decision boundaries.
Depending on whether they affect the model's groundtruth label, they can be categorized into \emph{adversarials} (\eg \citet{wu2019conditional} for training and \citet{jin2020bert} for evaluation) and \emph{counterfactuals} (\eg \citet{kaushik2019learning} for training and \citet{gardner2020contrast} for evaluation).
In the adversarial cases, the groundtruth label for $\xp$ are usually automatically determined and maintained, \ie $\yp = y$, whereas counterfactual ones require manual labeling~\cite{Khashabi2020MoreBF}, or a mapping between the perturbation templates and the resulting labels~\cite{li2020linguistically}.

\textbf{Properties.}
Desired perturbations are usually~\cite{morris2020textattack}:
(1) \emph{Minimally edited}, or $\xp$ is similar enough to $x$ to be considered as its variation; and 
(2) \emph{Grammatically correct and coherent}, or $\xp$ resembles $x$ enough that it has a high probability of being in the original $\xset$.
Additionally, augmentations usually prioritize (3) \emph{diversity} --- a group of $x$s are preturbed using various strategies, and various initiations under the same strategy, such that they provide different constraints on finetuning decision boundaries.
On the other hand, evaluations and explanations require more (4) \emph{controlled} perturbations for systematic and targeted inspections.
The diversity and the controllability are two competing factors, and prior work focusing on certain applications follow one of two extremes.
Those that thrive in diversity are either too uncontrolled (\eg text generation~\cite{iyyer2018adversarial}) or hard to scale (\eg manual rewrites~\cite{kaushik2019learning, gardner2020contrast}), whereas those that rely on templates or heuristic rules usually only cover limited linguistic patterns~\cite{li2020linguistically}.



\newcommand{\tagdefine}[1]{\emph{{\color{darkgray}#1} }}
\renewcommand{\arraystretch}{1.1}
\begin{table*}
\small
\centering
\begin{tabular}{p{0.11\linewidth} p{0.6\linewidth}  p{0.2\linewidth}}
\toprule
\textbf{\Tagstr} & \textbf{Definitions and Examples} & \textbf{Training datasets} \\ 
\midrule
\ctrltag{negation}
    & A dog is \add{not} embraced by the woman.
    & \cite{kaushik2019learning, gardner2020contrast}
\\ \midrule
\ctrltag{quantifier}
    & \swap{A dog is}{Three dogs are} embraced by the woman. 
    & \cite{gardner2020contrast}
\\ \midrule
\ctrltag{lexical}
    & \tagdefine{Changing just one word or noun chunks without breaking the POS tags.} \newline
      A dog is \swap{embraced}{attacked} by the woman.
    & \cite{sakaguchi2019winogrande}
\\ \midrule
\ctrltag{resemantic}
    & \tagdefine{To replace short phrases or clauses without affecting the parsing tree.}\newline
      A dog is \swap{embraced by the woman}{wrapped in a blanket}.
    & \cite{wieting2017paranmt}
\\ \midrule
\ctrltag{insert}
    & \tagdefine{To add constraints without affecting the parsing structure of other parts.} \newline
      A dog is embraced by the \add{little} woman.
    & \cite{wieting2017paranmt}
\\ \midrule
\ctrltag{delete}
    & \tagdefine{To remove constraints without affecting the parsing structure of other parts.} \newline
    A dog is embraced \remove{by the woman}.
    & \cite{wieting2017paranmt}
\\ \midrule
\ctrltag{restructure}
    & \tagdefine{To alter the dependency tree structure, \eg changing from passive to positive.} \newline
    A dog is \swap{embraced by}{hugging} the woman.
    & \cite{zhang2019paws, mccoy2019right}
\\ \midrule
\ctrltag{shuffle}
    & \tagdefine{To move (or swap) key phrases or entities around the sentence.} \newline
    A \swap{dog}{woman} is embraced by the \swap{woman}{dog}.
    & \cite{zhang2019paws, mccoy2019right}
\\
\bottomrule
\end{tabular}
\caption{A list of \tagstrs used for semantically driving the GPT-2 generation, their corresponding examples, and the training datasets that contains most of the corresponding patterns.
All the \add{perturbed examples} are generated by our finetuned model.}
%\wts{Change all the examples to be on an identical sentence, not all different cases. And consider further annotate the tags based on whether they just do semantic change or also syntactic change.}}
\label{table:ctrltag}
\end{table*}


\begin{figure}[t]
\centering
\includegraphics[width=1\columnwidth]{example-image-a}
\vspace{-15pt}
\caption{Given a sentence, get 3 different versions.}
\vspace{-10pt}
\label{fig:blank}
\end{figure}
\subsection{Perturbation as a NLG Task}

\paragraph{LM \& its prompts.}
In response to the objectives, we form the perturbation as a text generation task using language models (LM), which predict the continuing text given preceding prompts.
Here, instead of their common use case, \ie generating the remaining paragraphs, we use the original sentence $x$ as the prompt, and finetune off-the-shelf LMs to generate $\xp$: \exinline{}.
We further modify the prompt structure as the following:

\emph{Enhance diversity with \tagstrs}.
LMs are naturally cover more strategies than small collections of templates or rewrite rules, but to further extend their perturbation diversity, we condition the generation on special tokens that denote the perturbation strategies (similar to \citet{raffel2019exploring, Dathathri2020Plug}).
We surveyed X papers related to text perturbations, and summarized 8 \tagstrs as in Table~\ref{table:ctrltag}.

\emph{Enable control with \BLANK}.
Besides boosting diversity, the \tagstrs also enable controls on \emph{how} to edit a sentence.
We further borrow the blank fill-in structure~\cite{donahue2020enabling} to control \emph{where} to edit, \ie targeted subphrases are replaced with special \BLANK tokens, and the actual content (``answers'' to blanks) are concatenated to the end. 

Note that to train a general-purpose perturbation model, we do not distinguish adversarials and counterfactuals, as they depend on the NLP tasks where the perturbations will be used --- Adding an adjective modifier ``blue'' may not affect the sentiment of a sentence, but may affect the labels in questions answering or natural language inferences.
Later in this paper, we mostly demonstrate applications related to manually labeled counterfactuals, but we believe the method can be easily generated to the adversarial cases using some common filtering constraints (\eg sentence semantic similarity~\cite{morris2020textattack}).


\paragraph{Training data formation.}

While we do not have a single NLP dataset designed for strategy-driven perturbation, multiple existing datasets can each express a subset of the perturbation strategy patterns.
We survey multiple datasets with paired sentences, and combined six of them into a training set that cover all \tagstrs.
We additionally crawled some sentence pairs from non-paired datasets like SQuAD~\cite{rajpurkar-etal-2016-squad} to boost some specific patterns and increase lexical diversity. 
We list some of the representative datasets for each \tagstr in Table~\ref{table:ctrltag}, and describe them in more details in \S\ref{appendix:train_data}.

Given a pair, we use the two sentences interchangeably as $x$ and $\xp$ to learn the perturbations both ways.
We compute its primary \tagstr based on linguistic features like part-of-speech tagging or dependency trees, and blank out the changed subphrases in $\xp$.
If we cannot identify the \tagstr for a pair or if the editing distance is too large, we denote it with \ctrltag{[global]} and use them as negative samples.
Importantly, to allow flexible blanking at the generation time, instead of merely blanking the edited spans, we also extend the blank to cover their associated parsing structures, etc.
As a result, we form up to four unique training prompts given one $(x, \xp)$ pair (Figure~\ref{fig:blank}).

%max_log_prob_diff

We put \tagstrs before $\xp$ because we would like to prioritize \emph{how} over \emph{where}, such that when given a \tagstr, the model can determine the appropriate changing places, and generate the blanked prompt on its own. 
However, in cases where it is more essential to inspecting particular subphrases, it is possible to reverse the order, so we can provide the blanked prompts, and let the model figure out the \emph{how}.
It is also possible to swap the order of the blanked $\xp$ and the \tagstrs. 

With the interchangeable orders and the blanks, we generate 640,912 training prompts from 191,415 sentence pairs.
We use the data to finetune an off-the-shelf GPT-2 model (from \citet{Wolf2019HuggingFacesTS}), but any LM can potentially be used.
\wts{Add the training hyperparameters; And maybe eventually move them to }
We finetuned the model for 3 epochs, with an initial learning rate 5e-5, a batch size of 16 and a sequence length of 120.

\paragraph{Filtering on the generations.}
The infilling structure does not always generate \emph{valid} sentences, and therefore require additional filtering.
Inspired by the language model constraints in \citet{morris2020textattack}, we score both $x$ and $\xp$ using un-finetuned GPT-2, and invalidate all $\xp$ whose log-probability (either on the full sentence or on the perturbed chunks) decreases more than $k=10$ points compared to $x$ (though their other constraints may also be useful.)


%pr["pr_sent"]<=10 and pr["pr_phrase"] <=10
%Perplexity is instantiating; Language modeling as an approximation because it measures real world distribution.


\subsection{Intrinsic Evaluations \& Ablation Studies}
% random no-pair models. 1142508
We verify the impact of the filtering and the \tagstrs through human evaluations.
\paragraph{Validity.}
One of the coauthors manually labeled the validity of 600 augmentations generated on 120 instances from three datasets: \dsst~\cite{socher2013recursive}, \dnli~\cite{bowman-etal-2015-large}, and \dqqp~\cite{wang2018glue}.
The validity rate among all the generated instances were $61.5\%$, which increased to $78.3\%$ after the filtering. 
As part of the data labeling task in \S\ref{sec:app_label}, we also asked crowdworkers to label whether a perturbed sentence is valid (\emph{``whether the sentence is grammatically correct and semantically meaningful, likely written by a native English speaker''}), and they arrived at similar validation rates ($75.0\%$ for \dsst, $70.0\%$ for \dqqp, and $81.7\%$ for \dnli).
%If no filter at all, the instances you see will only have 60% valid stuff (we care more about precision, every invalid counterfactual shown to a human is wasted time):
\wts{Maybe no need for the instruction; It's in appendix.}


\paragraph{Controllability.}
One of the coauthors manually labeled ...\wts{this is TODO.}
the validity of 600 augmentations generated on 120 instances from three datasets.
Note that we only labeled instances generated with $\xp$ fully \BLANK-ed or being empty, so the model has the potential of fully follow the \tagstrs.
Further restricting the blank placements will lower the control rate, as it may be impossible to follow the strategy at the given blanked place (\eg \ctrltag{shuffle} is absolutely impossible when we only blank out a single token in the sentence.)



\paragraph{Diversity.}






