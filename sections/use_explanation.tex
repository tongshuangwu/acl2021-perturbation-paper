\section{Perturbation Explanations}

%Both counterfactual explanations and semi-counterfactual explanations.
%As defined in \cite{}

\subsection{Local explanation: ``surprises'' to SHAP}

Perturbations as counterfactual explanations are valuable, as they offer more information that goes beyond the mere input.
However, too many counterfactual explanations can be perceptually overwhelming.
As a result, existing methods on text are limited to showing a single decision boundary, which in turn losses easier overviews provided by feature attribution methods like SHAP or LIME.
Here, instead of displaying perturbations independently, we use them as supplemental \emph{surprise cases} to SHAP.

As a walkthrough example, consider the following \dnli instance.
The model~\cite{} predicts it correctly and SHAP deems the.

\wts{decide if we want to change the example...There are a lot of examples flying around in this paper.}

\ebox{
\textbf{Premise}: The quarterback of the UTEP football team is about to be tackled by a member of the Wisconsin defensive team.\\
\textbf{Hypothesis}: The \fwtwo{quarterback} is about to be \fwone{tackled} \fwtwo{by} the \fwthree{opposing team}.\\
%\textbf{Label}: entailment\\
\textbf{Prediction} $f(x)$: entailment
}

%%%
We define two kinds of surprises.
First, \emph{invariance surprises} (\emph{ivs}) refers to $\xp$-s that maintains model predictions, even though the perturbed features/tokens are deemed important by the model.
For example, the following changes three important tokens, ``quarterback,'' ``opposing'' and ``team,'' yet the predict remains intact. This contrasting example suggests that the model may not handle the subjects and objects of verbs appropriately.

\ebox{
\textbf{Hypothesis}: The \swap{quarterback}{opposing team} is about to be tackled \remove{by the opposing team}.\\
\textbf{Prediction} $f(\xp)$: entailment
}

Formally, if we denote the SHAP feature weight of a token $t$ as $w(t)$, then the degree of invariance surprise $S_i$ for $\xp$ is: 
$$S_i(\xp) = \mathbb{1}(f(x) = f(\xp)) \cdot \sum_{t \in r(\xp)} w(t) $$
And we can select the most surprising invariance $\xp$ by taking $\argmax_{\xp} S_i(\xp)$

%%%%
On the other hand, \emph{variance surprises} refers to $\xp$-s with changed model predictions when only unimportant features are perturbed.
Though ``tackled'' is not highlighted by SHAP, some replacements (\eg ``thrown'') does affect the prediction:

\ebox{
\textbf{Hypothesis}: The quarterback is about to be \swap{tackled}{thrown} by the opposing team.\\
\textbf{Prediction} $f(\xp)$: contradiction
}

Similar to above, variance surprise score $S_v$ can be computed by:
$$S_v(\xp) = \mathbb{1}(f(x) \not\eq f(\xp)) \cdot (1-\sum_{t \in r(\xp)} w(t)) $$
These surprises act as \emph{critics} to the original SHAP.
Note that they reveal abnormality of the \emph{explainer}, but not necessarily the abnormality of the model.
In the example above, we may suspect the model to miss verbs if we merely rely on SHAP explanations. However, the variance surprise shows otherwise --- the low importance results from SHAP only \emph{masking/deleting} the said verb.



\paragraph{Evaluate the ``surprise.''}
We validate the effect of the surprises through a user study with X MTurk crowdworkers.
Each worker was asked to finish N rounds of labeling.
In each round, they are given an original example from \dnli, the model prediction on it, and the SHAP importance (displayed as inline highlights.)
The are asked to simulate the model's prediction on 4 perturbations, which are the top two invariance surprises and two variance surprises.
We measure the accuracy of their simulation~\cite{hase2020evaluating}. 
The lower the accuracy, the more 
Similar to the labeling task in \S\ref{subsec:label_procedure}, we filter workers based on their label distribution and completion time.

\wts{baseline and results. See the comment for examples generated by BERT.}


\begin{comment}
****
The examples generated by BERT.
****

  P: The quarterback of the UTEP football team is about to be tackled by a member of the Wisconsin defensive team.
  H: The quarterback is about to be tackled by the opposing team.
 Pr: entailment
 NP: Another quarterback is about to be tackled by the opposing team.
NPr: neutral
weight:  0.122
flip_unimportant_feature 0.013 {The}

  P: The quarterback of the UTEP football team is about to be tackled by a member of the Wisconsin defensive team.
  H: The quarterback is about to be tackled by the opposing team.
 Pr: entailment
 NP: Jack is about to be tackled by the opposing team.
NPr: neutral
weight:  0.461
flip_unimportant_feature 0.028 {The, quarterback}


  P: The quarterback of the UTEP football team is about to be tackled by a member of the Wisconsin defensive team.
  H: The quarterback is about to be tackled by the opposing team.
 Pr: entailment
 NP: The quarterback is about to be tackled by someone
NPr: entailment
weight:  0.218
unflip_important_feature 0.3 {team, the, ., opposing}

  P: The quarterback of the UTEP football team is about to be tackled by a member of the Wisconsin defensive team.
  H: The quarterback is about to be tackled by the opposing team.
 Pr: entailment
 NP: The quarterback is about to be tackled by the second team.
NPr: entailment
weight:  0.292
unflip_important_feature 0.149 {opposing}


\end{comment}


\subsection{Extend ``Suprise'': Global, Interaction}
\paragraph{Global patterns.}
Going beyond surprises for each individual data points, we can use the \emph{surprise} to find perturbation patterns that systematically affect (preserve) model predictions.
Similar to challenge sets, the grouped perturbations then become useful for revealing model's capabilities on specific linguistic patterns.

To enable the grouping, we first featurize each perturbation $\xp$ with respect to its original instance $x$, using 
(1) its \tagstr \fexptag{\emph{s}}, 
(2) its remove phrases \fexpfrom{$r(\xp)$}, and 
(3) its added phrases \fexpto{$a(\xp)$}.
For tokens involving multiple changes, we featurize both the primary change, and the combined changes, and so the example in Figure~\ref{fig:blank} have the following features:

For each $x \in \xset$, we compute the surprise score for all its perturbations $\xp$-s, and denote an example as ``surprising'' based on thresholds, \ie $S_i(\xp) > \gamma_i$ or$S_v(\xp) > \gamma_i$ (empirically picked as $\gamma_i=0.3$ and $\gamma_v=0.7$).
We count the (in-)variance surprising rate $r_i$ and $r_v$ for each feature, \ie the ratio of surprising cases out of all perturbations who has the corresponding feature, and whose predictions are (not) changed.
We go through features with the surprise ratio $> 0.8$ to inspect global patterns.

Analyzing the \nli model mentioned above, we note some interesting observations that suggest model deficiencies:

(1) The model does not handle the shuffled phrases, echoing the results from \citet{mccoy2019right}.

\ebox{
\fexptag{shuffle}, $r_i=0.87$ (144 surprises / 164 invariance)\\
\textbf{Hypothesis}: The quarterback is about to be \swap{tackled}{thrown} by the opposing team.\\
\textbf{Hypothesis}: The quarterback is about to be \swap{tackled}{thrown} by the opposing team.\\
\textbf{Prediction}: \swap{entailment}{}
}

(2) 

We also note some valid model behaviors that are denoted ``surprising'', suggesting patterns that are frequently missed by SHAP: 



\paragraph{Interactive explanation.}
Besides pre-selected \emph{surprise} cases, perturbations can also support interactive explanation. 

