\section{Related Work}
\label{sec:relate}

Some prior work in counterfactual training and evaluation relies on humans to generate counterfactuals from scratch~\cite{gardner2020contrast, teney2020learning, kaushik2019learning}. 
Our experiments in \S\ref{sec:app_label} indicate that a hybrid approach where humans \emph{label} \sysname{} counterfactuals displays similar or better results at a much lower cost, and that it may be worth exploring a mixture of manual and automated generation. 
Similarly, prior work on analysis relies on experts to create individual counterfactuals or perturbation functions~\cite{wu2019errudite, checklist:acl20}. 
In \S\ref{sec:app_err_analysis}, we show how \sysname{} can enhance current practice by generating multiple counterfactuals that might have been overlooked, and by providing abstractions that allow for new kinds of analyses.

At the other end of the spectrum, prior work on automatically generating counterfactuals typically narrows its scope in terms of the relationships $x \veryshortarrow \xp$.
For example, adversarial generators aim to maintain semantics while changing model predictions~\cite{ribeiro2018semantically, iyyer2018adversarial, alzantot-etal-2018-generating}.
Meanwhile, concurrent work to our own~\cite{madaan2020generate, ross2020explaining} automatically generate $\xp$ that change predictions for the purpose of explanation or training, with no constraints on semantics.
However, as many of our examples in \S\ref{sec:app_label}--\S\ref{sec:app_err_analysis} show, a mix of label-preserving and label-flipping counterfactuals generated by \sysname is quite useful for training, evaluation, explanation, and analysis. 
Further, general-purpose counterfactuals may lead to serendipitous discoveries (\S\ref{sec:app_err_analysis}), especially as \sysname is not fine-tuned to the target domain (and thus less liable to merely replicate what is already there).
Finally, by allowing control through \tagstrs and \texttt{[BLANK]}s, \sysname{} supports human-generator collaborations, where the human specifies the desired changes (\eg change predictions \emph{through negation}).
Such collaboration is difficult to imagine with automatic generators.
Similarly, editing-based text style transfer only perturbations based on predefined style attributes~\cite{madaan-etal-2020-politeness, malmi-etal-2020-unsupervised}, with no finer-grained controls.


% Counterfactuals in various applications differ by their impact on the groundtruth and model prediction labels.
% Those changing the groundtruth are mostly used for counterfactual training~\cite{kaushik2019learning, teney2020learning} and evaluation~\cite{gardner2020contrast, checklist:acl20}.
% As mentioned in \S\ref{sec:intro}, these counterfactuals rely heavily on human creativity and are expensive to collect.
% Though it is cheaper to automate the process with parsing templates~\cite{li2020linguistically}, usually the templates are only applicable to a small subset of data.
% \citet{wang2020robustness} also proposed to automatically flip the groundtruth by replacing causal word features with their antonyms.
% However, they acknowledged that the method supplies limited tasks, as ``antonym'' is only well-defined in sentiment analysis.
% We achieve a middle ground, showing that the team of \sysname and a human moderator collect data more effectively.

% On the other hand, multiple prior works automatically generate counterfactuals that flip model predictions.
% These methods are used for counterfactual explanations.
% For example, \citet{kang2020counterfactual} guided word replacement with model prediction probabilities.
% Our concurrent work, GYC~\cite{madaan2020generate} and MiCE~\cite{ross2020explaining} further enable phrase replacement through language models (GPT-2 or t5).
% However, these methods leave out prediction-persistent explanations, which are also important as shown in \S\ref{sec:app_explain}.
% If collected with additional constraints on semantic persistence~\cite{morris2020textattack, alzantot-etal-2018-generating}, the prediction-flipping counterfactuals become adversarial examples.
% Despite the apparent similarities between explanations and adversarials, to the best of our knowledge, adversarial generation is still mostly through word replacement~\cite{alzantot2018generating, garg2020bae, li2020contextualized} or paraphrasing~\cite{iyyer2018adversarial, malandrakis-etal-2019-controlled}, and has yet to take advantage of language models.
% Given the shared properties, a general-purpose model like \sysname may be more reasonable than methods tailored to specific applications.

% Moreover, the emphasis on \emph{labels} compresses a large set of diverse changes (\eg negations and antonym replacements both flip labels in sentiment analysis), and therefore hinders the definition and queryiig of desired counterfactuals.
% We instead design \sysname's controls to be task (label)-agnostic and finer-grained.
% Such controls additionally support applications that require specifications on where and how to perturb, \eg open-ended error analysis~\cite{wu2019errudite} where contrasting similar counterfactuals is essential (\S\ref{sec:app_err_analysis}).


