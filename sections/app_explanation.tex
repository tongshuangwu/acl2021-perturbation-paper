\section{Additional Details to Explanations \S\ref{sec:app_explain}}
\label{appendix:explanation}


\subsection{Selection Methods}
\label{appendix:exp_rank}

\paragraph{SHAP complement as local explanation}
Because the SHAP weights reflect the average effect of masking a token $t$, instead of finding just one abnormal counterfactual, we focus on \emph{word features that are abnormal on average}, \ie there is a mismatch between the importance of the changed word features and the model behavior, averaged over \emph{all counterfactuals} that have those features.

In this case, the expected change in prediction of perturbing a token $t$ is the SHAP importance on it $\E[\dist(t, x)] = s(t)$; 
For example, in Figure~\ref{fig:explanation}, $s(t=\text{``depression''})=0.276$.

The actual prediction change is the weighted average of $|\fp(x)-\fp(\xp)|$ for all the $\xp$ that affect $t$ (\swap{depression}{trouble}, \swap{depression}{a relationship}), with the weight corresponding to the number of words modified in $\xp$: If $e(\xp)$ denotes the set of edited words (replaced or deleted from $x$ in $\relation{\xp}$), then $w(\xp) = 1/|e(\xp)|$.
Intuitively, the more words changed in $\xp$, the less effect each word has; In Figure~\ref{fig:explanation}D, we regard ``depression'' to be responsible for half of the impact on changing \swap{in depression}{suicidal}.

We group the counterfactuals based on their affected words $G_t = \{\xp\ |\ t \in e(\xp)\}$. $\dist(t, x)$ then becomes:
$$\dist(t, x) = \frac{1}{|G_t|+1} \left(s(t) + \sum_{\xp \in G_t} w(t)\cdot |\fp(x)-\fp(\xp)|\right)$$
We add the additional SHAP weight $s(t)$ as a smoothing factor to punish dominating outlier $\xp$.
Then the gap between the expectation and the reality is (similar to \S\ref{subsec:local_explain}):
$$\Delta\dist(t, x) = \dist(t, x)-\E[\dist(t, x)]$$
We first find the abnormal tokens: (1) $t$ with small SHAP weight, but $\xp$ that change $t$ experience large prediction change on average: $t_L = \argmax_{t\in x} \Delta\dist(t, x)$, and (2) $t$ with large SHAP weight, but $\xp$ with $t$ changed usually have intact prediction: $t_U = \argmax_{t\in x} -\Delta\dist(t, x)$.

Then, we use the most extreme cases within the groups of $G_{t_L}$ and $G_{t_U}$ as the concrete counterfactual explanations, based on their prediction change $|\fp(x)-\fp(\xp)|$, and the aggregated SHAP weights of all the changed tokens:
$$\xp_L = \argmax_{\xp \in G_{t_L}} \left( |f_p(x)-f_p(\xp)| - \sum_{u\in r(\xp)} s(u) \right)$$ 



\paragraph{Global explanation}
To enable the grouping, we first featurize each counterfactual $\xp$ with respect to its original instance $x$, using 
(1) its \tagstr (\fexptag{negation} for the example in Figure~\ref{fig:blank}), 
(2) its remove phrases \fexpfrom{kids}, 
(3) its added phrases \fexpto{not}, \fexpto{children}, and 
(4) the combined template \fexptemp{\swap{kids}{children}}.
For tokens involving multiple changes, we featurize both the primary and the combined changes, and so the example in Figure~\ref{fig:blank} also have additional features like \texttt{\fexptag{negation} \& \fexptag{lexical}}.
These features form the relationship $\relation{\xp}$ in use.

For each feature $h \in \relation{\xp}$, we build a group of counterfactuals that contain $h$ (of course, these also include their corresponding $x$): $G_h = \{ \xp\ |\ h\in \relation{\xp} \}$.
We compute the probability of the predicted label changes all the $\xp \in G_{h}$: $Pr(y_1, y_2) = |G_h^{y_1\veryshortarrow y_2}|/|G_h|$, where $ G_h^{y_1\veryshortarrow y_2} = \{ (x, \xp)\ |\ (x, \xp) \in G_h, f(x)=y_1, f(\xp) = y_2 \}$.
The abnormality of a feature $h$ is represented by the entropy of the prediction change:
$$I_h = -\sum_{y_1 \in Y, y_2 \in Y} Pr(y_1, y_2) \cdot \log Pr(y_1, y_2)$$
We find the abormal feature with $\hat{h} = \argmin I_h$.



\subsection{User Study Details}
\label{appendix:exp_user_study}

The instruction for the user study in \S\ref{subsec:exp_user_study} is in Figure~\ref{fig:explanation_instruction}, and Figure~\ref{fig:explanation_ui} shows the sample interface for one round. 
Participants started by just seeing the reference example and the model query box on the left hand side.
When they chose to start the task or after they had exhausted their ten query chances, the query box was disabled, the tasks on the right were displayed, and the participants complete the tasks.